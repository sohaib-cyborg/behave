# -*- coding: utf-8 -*-
"""twitter-data-sentiment-analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KAR8I2HaL7FRVAIRcmhfsSGYwk6I-R4I
"""

import tweepy
import pandas as pd
from transformers import pipeline
import os
from datetime import datetime, timedelta
import numpy as np

# 1. Generate query variants for any given coin
def generate_coin_queries(coin_name: str, ticker: str):
    """
    Generates a list of query variants for a given cryptocurrency coin.
    This includes the ticker, name, and common crypto slang terms.
    """
    variants = {
        ticker,
        # f"${ticker}", # Using $ with recent tweets is often problematic
        coin_name, coin_name.lower(), coin_name.upper(),
        f"#{coin_name}", f"#{ticker}",
        # f"${ticker}USD",
        f"#{coin_name}Price"
    }
    # Simple misspelling strategy: drop one character
    for i in range(len(coin_name)):
        missp = coin_name[:i] + coin_name[i+1:]
        variants.add(missp.lower())
    # Leetspeak example
    leet = coin_name.replace('O', '0').replace('I', '1').replace('E', '3')
    variants.add(leet)
    # Crypto slang terms
    variants.update({"HODL", "FOMO", "DYOR", "WAGMI", "moon", "rugpull", "rekt"})
    return list(variants)

# 2. Initialize Tweepy client
# NOTE: The bearer_token must be set in your Colab user data or environment.
# bearer_token = userdata.get('BEARER_TOKEN')
# For this example, let's assume a token is available.
# Replace with your actual token
bearer_token = os.environ.get('BEARER_TOKEN') or "YOUR_BEARER_TOKEN_HERE"
client = tweepy.Client(bearer_token=bearer_token)

# 3. Initialize sentiment analyzer using FinTwitBERT-sentiment
sentiment_analyzer = pipeline(
    "sentiment-analysis",
    model="StephanAkkerman/FinTwitBERT-sentiment"
)  # Specialized for financial tweets

def fetch_tweets_in_range(coin_name: str, ticker: str, max_tweets: int, start_date: datetime, end_date: datetime):
    """
    Helper function to fetch tweets within a specified date range.
    It works by iteratively calling search_recent_tweets and moving the end date back
    until the start date is reached.
    """
    query = " OR ".join(generate_coin_queries(coin_name, ticker))
    query += " -is:retweet lang:en"

    all_tweets = []
    current_end_time = end_date

    while current_end_time > start_date and len(all_tweets) < max_tweets:
        print(f"Fetching tweets up to {current_end_time.isoformat()}...")
        # search_recent_tweets only supports fetching up to 100 tweets per call.
        # It also has a 7-day lookback window, meaning `end_time` must be in the last 7 days.
        # This function simulates a longer history search by manually iterating.
        response = client.search_recent_tweets(
            query=query,
            max_results=min(max_tweets - len(all_tweets), 100),
            end_time=current_end_time,
            tweet_fields=["created_at"]
        )
        
        tweets = response.data or []
        all_tweets.extend(tweets)

        if not tweets:
            print("No more tweets found in this time range. Stopping.")
            break

        # Find the oldest tweet's timestamp to set the next search's end time
        oldest_tweet_time = min(tweet.created_at for tweet in tweets)
        current_end_time = oldest_tweet_time - timedelta(seconds=1)

        print(f"Fetched {len(tweets)} tweets. Total tweets so far: {len(all_tweets)}.")

    return all_tweets


def fetch_and_analyze(coin_name: str, ticker: str, max_tweets: int = 100, start_date: str = None, end_date: str = None):
    """
    Fetches tweets, performs sentiment analysis, and returns a DataFrame.
    Can now handle a date range using the start_date and end_date parameters.
    """
    if start_date and not end_date:
        raise ValueError("If start_date is provided, end_date must also be provided.")
    if end_date and not start_date:
        # If only end_date is provided, we fetch recent tweets up to that time.
        # Note: The end_date must be within the last 7 days for the search_recent_tweets API.
        end_dt = datetime.fromisoformat(end_date)
        if end_dt.date() < (datetime.now() - timedelta(days=7)).date():
            print("Warning: `end_date` must be within the last 7 days for a single search.")

        query = " OR ".join(generate_coin_queries(coin_name, ticker))
        query += " -is:retweet lang:en"

        params = {
            "query": query,
            "max_results": max_tweets,
            "end_time": end_dt
        }
        
        response = client.search_recent_tweets(**params)
        tweets = response.data or []
    elif start_date and end_date:
        # If both dates are provided, use the new helper function for the range
        try:
            start_dt = datetime.fromisoformat(start_date)
            end_dt = datetime.fromisoformat(end_date)
        except ValueError:
            raise ValueError("Dates must be in ISO format: YYYY-MM-DD or YYYY-MM-DDTHH:MM:SS")
        
        tweets = fetch_tweets_in_range(coin_name, ticker, max_tweets, start_dt, end_dt)
        print(f"Total tweets fetched for the range: {len(tweets)}")
    else:
        # Default behavior: fetch recent tweets without a specific date range
        query = " OR ".join(generate_coin_queries(coin_name, ticker))
        query += " -is:retweet lang:en"
        response = client.search_recent_tweets(query=query, max_results=max_tweets)
        tweets = response.data or []

    if not tweets:
        print("No tweets found.")
        return pd.DataFrame(columns=["text", "label", "score"])

    texts = [tweet.text for tweet in tweets]
    results = sentiment_analyzer(texts)
    data = []
    for tweet, res in zip(tweets, results):
        data.append({
            "text": tweet.text,
            "label": res["label"],
            "score": res["score"],
            "created_at": tweet.created_at # Adding created_at for context
        })
    return pd.DataFrame(data)

def aggregate_sentiment(df: pd.DataFrame):
    """
    Aggregates the sentiment scores from a DataFrame of analyzed tweets.
    Returns a dictionary with the overall sentiment and confidence score.
    """
    if df.empty:
        return {"overall_sentiment": "NEUTRAL", "overall_confidence": 0.0}

    label_map = {"BULLISH": 1, "NEUTRAL": 0, "BEARISH": -1}
    df['numeric_label'] = df['label'].map(label_map)

    df['weighted_score'] = df['numeric_label'] * df['score']

    total_weighted_score = df['weighted_score'].sum()
    total_confidence = df['score'].sum()

    if total_confidence == 0:
        return {"overall_sentiment": "NEUTRAL", "overall_confidence": 0.0}

    aggregated_score = total_weighted_score / total_confidence

    # Define thresholds for sentiment classification
    if aggregated_score > 0.1:
        overall_sentiment = "BULLISH"
    elif aggregated_score < -0.1:
        overall_sentiment = "BEARISH"
    else:
        overall_sentiment = "NEUTRAL"

    overall_confidence = abs(aggregated_score)

    return {
        "overall_sentiment": overall_sentiment,
        "overall_confidence": overall_confidence
    }

# Example usage
if __name__ == "__main__":
    # Example 1: Fetching recent tweets with an end date
    print("--- Example 1: Fetching recent tweets up to a specific date ---")
    df_recent = fetch_and_analyze("Ethereum", "ETH", max_tweets=50, end_date="2025-08-12T00:00:00")
    print("Per-tweet sentiment analysis:")
    if not df_recent.empty:
        print(df_recent.head())
        aggregated_result_recent = aggregate_sentiment(df_recent)
        print("\nOverall aggregated sentiment:")
        print(f"Sentiment: {aggregated_result_recent['overall_sentiment']}")
        print(f"Confidence: {aggregated_result_recent['overall_confidence']:.4f}")
    else:
        print("No tweets found for this example.")

    print("\n" + "="*50 + "\n")

    # Example 2: Fetching tweets over a historical date range
    print("--- Example 2: Fetching tweets over a historical date range ---")
    # Note: For this to work with Twitter's API, the start_date and end_date
    # must be within the last 7 days for the `search_recent_tweets` endpoint.
    # The helper function simulates a longer search, but the API itself
    # has this inherent limitation.
    start = (datetime.now() - timedelta(days=3)).isoformat()
    end = datetime.now().isoformat()
    df_range = fetch_and_analyze("Bitcoin", "BTC", max_tweets=150, start_date=start, end_date=end)
    print("Per-tweet sentiment analysis:")
    if not df_range.empty:
        print(df_range.head())
        aggregated_result_range = aggregate_sentiment(df_range)
        print("\nOverall aggregated sentiment:")
        print(f"Sentiment: {aggregated_result_range['overall_sentiment']}")
        print(f"Confidence: {aggregated_result_range['overall_confidence']:.4f}")
    else:
        print("No tweets found for this date range.")
